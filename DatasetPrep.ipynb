{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaaagri/iat481-cv-proj/blob/wip/DatasetPrep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is based on Week 6 tutorial, we use it to check, enumerate, analyse our datasets in terms of correct labeling and balance, before merging them and doing the training with YOLO."
      ],
      "metadata": {
        "id": "7Le1bvcPKxgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prerequisites:"
      ],
      "metadata": {
        "id": "8jbtjhzVKgid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/MyDrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcopSGdNKgSo",
        "outputId": "12d9b7e6-91f4-4a13-e749-b45ac45b9b24"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The labels for this dataset are stored in YOLO format. i.e.\n",
        "\n",
        "    class_id bbox_x_center bbox_y_center bbox_width bbox_height"
      ],
      "metadata": {
        "id": "4GmdTvHOLWCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our pest animals classes:"
      ],
      "metadata": {
        "id": "H-uKtcl2LiYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = {\n",
        "    0: \"bear\",\n",
        "    1: \"raccoon\",\n",
        "    2: \"rat\",\n",
        "    3: \"skunk\"\n",
        "}"
      ],
      "metadata": {
        "id": "dqGwCTe9LoIV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required libraries:"
      ],
      "metadata": {
        "id": "s5pmk-k3L7EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random as random\n",
        "import os"
      ],
      "metadata": {
        "id": "BReXY07pL8I8"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining paths to the collected datasets, both found and the ones we prepared ourselves:"
      ],
      "metadata": {
        "id": "cyOlfbRIMwTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_paths = {\n",
        "  \"found_bear\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Found/Bear/Bear.v1i.yolov8\",\n",
        "  \"found_raccoon\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Found/Raccoon/Raccoon.v38-416x416-resize.yolov8\",\n",
        "  \"found_rat\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Found/Rat/cc-object-detection.v6i.yolov8\",\n",
        "  \"found_skunk\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Found/Skunk/Skunk.v1i.yolov8\",\n",
        "  \"ours_bear\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Ours/Bear\",\n",
        "  \"ours_raccoon\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Ours/Raccoon\",\n",
        "  \"ours_rat\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Ours/Rat\",\n",
        "  \"ours_skunk\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Ours/Skunk\",\n",
        "}"
      ],
      "metadata": {
        "id": "G2OH8Mc_MsNB"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collect all image and label files we've got and save into one data structure:"
      ],
      "metadata": {
        "id": "V-ardTmZHkcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enumerate_dataset(dataset_id, dataset_path):\n",
        "  image_paths, label_paths = {}, {}\n",
        "\n",
        "  try:\n",
        "    # sorting the directories listings so that images' and labels' positions in lists match\n",
        "    image_paths['train'] = [os.path.join(dataset_path, \"train\", \"images\", image_filename) for image_filename in sorted(os.listdir(os.path.join(dataset_path, \"train\", \"images\")))]\n",
        "    image_paths['val'] = [os.path.join(dataset_path, \"valid\", \"images\", image_filename) for image_filename in sorted(os.listdir(os.path.join(dataset_path, \"valid\", \"images\")))]\n",
        "    image_paths['test'] = [os.path.join(dataset_path, \"test\", \"images\", image_filename) for image_filename in sorted(os.listdir(os.path.join(dataset_path, \"test\", \"images\")))]\n",
        "  except FileNotFoundError:\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    label_paths['train'] = [os.path.join(dataset_path, \"train\", \"labels\", image_filename) for image_filename in sorted(os.listdir(os.path.join(dataset_path, \"train\", \"labels\")))]\n",
        "    label_paths['val'] = [os.path.join(dataset_path, \"valid\", \"labels\", image_filename) for image_filename in sorted(os.listdir(os.path.join(dataset_path, \"valid\", \"labels\")))]\n",
        "    label_paths['test'] = [os.path.join(dataset_path, \"test\", \"labels\", image_filename) for image_filename in sorted(os.listdir(os.path.join(dataset_path, \"test\", \"labels\")))]\n",
        "  except FileNotFoundError:\n",
        "    pass\n",
        "\n",
        "  return image_paths, label_paths\n",
        "\n",
        "def enumerate_dataset_multi(dataset_paths):\n",
        "  return {dataset_id: enumerate_dataset(dataset_id, dataset_paths[dataset_id]) for dataset_id in dataset_paths}"
      ],
      "metadata": {
        "id": "MJY5CW1ZL_nw"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = enumerate_dataset_multi(dataset_paths)"
      ],
      "metadata": {
        "id": "yJaxxSXlUjTS"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to easy count the contents of a dataset by class across 'train', 'val', and 'test' parts\n",
        "def count_samples(dataset):\n",
        "  sample_counts = {}\n",
        "\n",
        "  for k, v in dataset.items():\n",
        "    sample_counts[k] = [len(v['images']['train']), len(v['images']['val']), len(v['images']['test'])]\n",
        "\n",
        "  return sample_counts\n",
        "\n",
        "# create a writeable place in the dataset dictionary for a class\n",
        "def initialize_class_in_dataset(cl, dataset):\n",
        "  dataset[cl] = {}\n",
        "  dataset[cl]['images'] = {}\n",
        "  dataset[cl]['images']['train'] = []\n",
        "  dataset[cl]['images']['val'] = []\n",
        "  dataset[cl]['images']['test'] = []\n",
        "  dataset[cl]['labels'] = {}\n",
        "  dataset[cl]['labels']['train'] = []\n",
        "  dataset[cl]['labels']['val'] = []\n",
        "  dataset[cl]['labels']['test'] = []"
      ],
      "metadata": {
        "id": "H0R4-tEAF7sc"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging all the data we have into N strict classes (defined earlier). Doing our best with the totality of data we've got to make sure the classes are balanced."
      ],
      "metadata": {
        "id": "lBpRnSb9Hobs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_ids = dataset_paths.keys()\n",
        "\n",
        "merged_dataset = {}\n",
        "\n",
        "for cl in class_labels.values():\n",
        "  for id in dataset_ids:\n",
        "    if cl in id:\n",
        "\n",
        "      if cl not in merged_dataset:\n",
        "        initialize_class_in_dataset(cl, merged_dataset)\n",
        "\n",
        "      # ignoring KeyError, as some datasets may have only a 'train' folder, but not the others\n",
        "      try:\n",
        "        merged_dataset[cl]['images']['train'] += datasets[id][0]['train']\n",
        "        merged_dataset[cl]['labels']['train'] += datasets[id][1]['train']\n",
        "      except KeyError:\n",
        "        pass\n",
        "\n",
        "      try:\n",
        "        merged_dataset[cl]['images']['val'] += datasets[id][0]['val']\n",
        "        merged_dataset[cl]['labels']['val'] += datasets[id][1]['val']\n",
        "      except KeyError:\n",
        "        pass\n",
        "\n",
        "      try:\n",
        "        merged_dataset[cl]['images']['test'] += datasets[id][0]['test']\n",
        "        merged_dataset[cl]['labels']['test'] += datasets[id][1]['test']\n",
        "      except KeyError:\n",
        "        pass\n",
        "\n",
        "# undersampling all classes to match the lowest amount of train samples available - this will result in the balance we look for\n",
        "train_samples_count = [v[0] for k, v in count_samples(merged_dataset).items()]\n",
        "min_train_samples_count = np.array(train_samples_count).min()\n",
        "\n",
        "balanced_dataset = {}\n",
        "\n",
        "for cl, v in merged_dataset.items():\n",
        "  if cl not in balanced_dataset:\n",
        "    initialize_class_in_dataset(cl, balanced_dataset)\n",
        "\n",
        "  # Fill the training set, capping at min_train_samples_count\n",
        "  balanced_dataset[cl]['images']['train'] = v['images']['train'][:min_train_samples_count]\n",
        "  balanced_dataset[cl]['labels']['train'] = v['labels']['train'][:min_train_samples_count]\n",
        "\n",
        "  # Take out the samples from the original merged dataset\n",
        "  merged_dataset[cl]['images']['train'] = v['images']['train'][min_train_samples_count:]\n",
        "  merged_dataset[cl]['labels']['train'] = v['labels']['train'][min_train_samples_count:]\n",
        "\n",
        "\n",
        "sample_pool = {}\n",
        "\n",
        "# try to reach 80/10/10 ratios for our train-val-test split\n",
        "val_test_target_sample_count = round(min_train_samples_count * 0.125)\n",
        "\n",
        "#print(val_test_target_sample_count)\n",
        "\n",
        "for cl, data in merged_dataset.items():\n",
        "  # dumping all leftover samples into one list per class\n",
        "  sample_pool = {}\n",
        "  sample_pool['images'] = []\n",
        "  sample_pool['labels'] = []\n",
        "\n",
        "  sample_pool['images'] += [i for i in data['images']['train']]\n",
        "  sample_pool['images'] += [i for i in data['images']['val']]\n",
        "  sample_pool['images'] += [i for i in data['images']['test']]\n",
        "  sample_pool['labels'] += [l for l in data['labels']['train']]\n",
        "  sample_pool['labels'] += [l for l in data['labels']['val']]\n",
        "  sample_pool['labels'] += [l for l in data['labels']['test']]\n",
        "\n",
        "  n = val_test_target_sample_count\n",
        "\n",
        "  # finally, populating our validation and testing set\n",
        "  balanced_dataset[cl]['images']['val'] = sample_pool['images'][:n]\n",
        "  balanced_dataset[cl]['labels']['val'] = sample_pool['labels'][:n]\n",
        "  balanced_dataset[cl]['images']['test'] = sample_pool['images'][n:n+n]\n",
        "  balanced_dataset[cl]['labels']['test'] = sample_pool['labels'][n:n+n]"
      ],
      "metadata": {
        "id": "kPJAuCzo7Idi"
      },
      "execution_count": 365,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the train-val-test distribution of samples in our final, merged and balanced dataset:"
      ],
      "metadata": {
        "id": "QyI5mrrBgKgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(count_samples(balanced_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUWVuD6JgZKM",
        "outputId": "eee8f811-16dd-408a-99af-9edf1814a7b8"
      },
      "execution_count": 366,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bear': [162, 20, 20], 'raccoon': [162, 20, 20], 'rat': [162, 20, 20], 'skunk': [162, 20, 20]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! Now, just let's make sure the dataset does not require any further cleaning, meaning there are no images with missing labels, or orphan labels that do not belong to any image:"
      ],
      "metadata": {
        "id": "nnmkXwbSg1bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_cleanness_check(dataset):\n",
        "  image_files = set()\n",
        "  label_files = set()\n",
        "\n",
        "  for cl, data in dataset.items():\n",
        "    for subset_type in ['train', 'val', 'test']:\n",
        "      image_files.update({i.split(\"/\")[-1].split(\".\")[0] for i in data['images'][subset_type]})\n",
        "      label_files.update({l.split(\"/\")[-1].split(\".\")[0] for l in data['labels'][subset_type]})\n",
        "\n",
        "  print(f\"Extra images (without corresponding labels): {image_files - label_files}\")\n",
        "  print(f\"Extra labels (without corresponding images): {label_files - image_files}\")"
      ],
      "metadata": {
        "id": "OMK4Hm26hNcO"
      },
      "execution_count": 369,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_cleanness_check(balanced_dataset)"
      ],
      "metadata": {
        "id": "rpXCmI_0hfHS",
        "outputId": "e9b2c355-37be-43e4-b01e-852bdad6a658",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extra images (without corresponding labels): set()\n",
            "Extra labels (without corresponding images): set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are none extra images or labels! Great. Now we are ready to save the merged dataset on disk and proceed to training."
      ],
      "metadata": {
        "id": "se-PmSsilSpO"
      }
    }
  ]
}