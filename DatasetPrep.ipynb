{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaaagri/iat481-cv-proj/blob/wip/DatasetPrep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is based on Week 6 tutorial, we use it to check, enumerate, analyse our datasets in terms of correct labeling and balance, before merging them and doing the training with YOLO."
      ],
      "metadata": {
        "id": "7Le1bvcPKxgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prerequisites:"
      ],
      "metadata": {
        "id": "8jbtjhzVKgid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/MyDrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcopSGdNKgSo",
        "outputId": "12d9b7e6-91f4-4a13-e749-b45ac45b9b24"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The labels for this dataset are stored in YOLO format. i.e.\n",
        "\n",
        "    class_id bbox_x_center bbox_y_center bbox_width bbox_height"
      ],
      "metadata": {
        "id": "4GmdTvHOLWCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our pest animals classes:"
      ],
      "metadata": {
        "id": "H-uKtcl2LiYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = {\n",
        "    0: \"bear\",\n",
        "    1: \"raccoon\",\n",
        "    2: \"rat\",\n",
        "    3: \"skunk\"\n",
        "}"
      ],
      "metadata": {
        "id": "dqGwCTe9LoIV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required libraries:"
      ],
      "metadata": {
        "id": "s5pmk-k3L7EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random as random\n",
        "import os"
      ],
      "metadata": {
        "id": "BReXY07pL8I8"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining paths to the collected datasets, both found and the ones we prepared ourselves:"
      ],
      "metadata": {
        "id": "cyOlfbRIMwTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_paths = {\n",
        "  \"found_bear\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Found/Bear/Bear.v1i.yolov8\",\n",
        "  \"found_raccoon\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Found/Raccoon/Raccoon.v38-416x416-resize.yolov8\",\n",
        "  \"found_rat\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Found/Rat/cc-object-detection.v6i.yolov8\",\n",
        "  \"found_skunk\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Found/Skunk/Skunk.v1i.yolov8\",\n",
        "  \"ours_bear\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Ours/Bear\",\n",
        "  \"ours_raccoon\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Ours/Raccoon\",\n",
        "  \"ours_rat\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Ours/Rat\",\n",
        "  \"ours_skunk\": \"/content/MyDrive/MyDrive/IAT481/481 CV Project/Datasets/Ours/Skunk\",\n",
        "}"
      ],
      "metadata": {
        "id": "G2OH8Mc_MsNB"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect all image and label files we've got and save into one data structure\n",
        "\n",
        "def enumerate_dataset(dataset_id, dataset_path):\n",
        "  image_paths, label_paths = {}, {}\n",
        "\n",
        "  try:\n",
        "    image_paths['train'] = [os.path.join(dataset_path, \"train\", \"images\", image_filename) for image_filename in os.listdir(os.path.join(dataset_path, \"train\", \"images\"))]\n",
        "    image_paths['val'] = [os.path.join(dataset_path, \"valid\", \"images\", image_filename) for image_filename in os.listdir(os.path.join(dataset_path, \"valid\", \"images\"))]\n",
        "    image_paths['test'] = [os.path.join(dataset_path, \"test\", \"images\", image_filename) for image_filename in os.listdir(os.path.join(dataset_path, \"test\", \"images\"))]\n",
        "  except FileNotFoundError:\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    label_paths['train'] = [os.path.join(dataset_path, \"train\", \"labels\", image_filename) for image_filename in os.listdir(os.path.join(dataset_path, \"train\", \"labels\"))]\n",
        "    label_paths['val'] = [os.path.join(dataset_path, \"valid\", \"labels\", image_filename) for image_filename in os.listdir(os.path.join(dataset_path, \"valid\", \"labels\"))]\n",
        "    label_paths['test'] = [os.path.join(dataset_path, \"test\", \"labels\", image_filename) for image_filename in os.listdir(os.path.join(dataset_path, \"test\", \"labels\"))]\n",
        "  except FileNotFoundError:\n",
        "    pass\n",
        "\n",
        "  return image_paths, label_paths\n",
        "\n",
        "def enumerate_dataset_multi(dataset_paths):\n",
        "  return {dataset_id: enumerate_dataset(dataset_id, dataset_paths[dataset_id]) for dataset_id in dataset_paths}"
      ],
      "metadata": {
        "id": "MJY5CW1ZL_nw"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = enumerate_dataset_multi(dataset_paths)"
      ],
      "metadata": {
        "id": "yJaxxSXlUjTS"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing our best with the totality of data we've got to make sure our classes are balanced\n",
        "\n",
        "dataset_ids = dataset_paths.keys()\n",
        "merged_dataset = {}\n",
        "\n",
        "for c in class_labels.values():\n",
        "  for id in dataset_ids:\n",
        "    if c in id:\n",
        "      # create a place in the dictionary for the next class\n",
        "      if c not in merged_dataset:\n",
        "        merged_dataset[c] = {}\n",
        "        merged_dataset[c]['images'] = {}\n",
        "        merged_dataset[c]['images']['train'] = []\n",
        "        merged_dataset[c]['images']['val'] = []\n",
        "        merged_dataset[c]['images']['test'] = []\n",
        "        merged_dataset[c]['labels'] = {}\n",
        "        merged_dataset[c]['labels']['train'] = []\n",
        "        merged_dataset[c]['labels']['val'] = []\n",
        "        merged_dataset[c]['labels']['test'] = []\n",
        "\n",
        "      # ignoring KeyError, as some datasets may have only a 'train' folder, but not the others\n",
        "      try:\n",
        "        merged_dataset[c]['images']['train'] += datasets[id][0]['train']\n",
        "        merged_dataset[c]['labels']['train'] += datasets[id][1]['train']\n",
        "      except KeyError:\n",
        "        pass\n",
        "\n",
        "      try:\n",
        "        merged_dataset[c]['images']['val'] += datasets[id][0]['val']\n",
        "        merged_dataset[c]['labels']['val'] += datasets[id][1]['val']\n",
        "      except KeyError:\n",
        "        pass\n",
        "\n",
        "      try:\n",
        "        merged_dataset[c]['images']['test'] += datasets[id][0]['test']\n",
        "        merged_dataset[c]['labels']['test'] += datasets[id][1]['test']\n",
        "      except KeyError:\n",
        "        pass\n",
        "\n",
        "\n",
        "for k, v in merged_dataset.items():\n",
        "  print(k, len(v['images']['train']), len(v['images']['val']), len(v['images']['test']))\n",
        "\n",
        "\n",
        "#file_counts = [len(datasets[id][0]['train']) for id in dataset_ids]\n",
        "\n",
        "#np.array(file_counts)\n",
        "#merged_dataset"
      ],
      "metadata": {
        "id": "kPJAuCzo7Idi",
        "outputId": "d26196a8-ad8d-46a4-ef84-723db8143c6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bear 1500 92 94\n",
            "raccoon 170 29 17\n",
            "rat 162 41 20\n",
            "skunk 220 0 0\n",
            "bear 1500 92 94\n",
            "raccoon 170 29 17\n",
            "rat 162 41 20\n",
            "skunk 220 0 0\n"
          ]
        }
      ]
    }
  ]
}